## Exercise: Design a Human-in-the-Loop Workflow

### Overview

In this exercise, you will design a **human-in-the-loop (HITL) workflow** that enables safe and accountable oversight of generative AI outputs before they are used in a real-world context. You will model how humans interact with AI-generated content, where review and decision-making occur, and how feedback is incorporated back into the system.

The goal is to move beyond the idea of “human review” as an abstract control and instead design a **clear, auditable workflow** that demonstrates responsibility, escalation paths, and effective collaboration between humans and AI.

You will use **Mermaid diagrams** to visually represent the workflow and annotate key decision points to explain how risk is reduced and accountability is maintained.

---

### Prerequisites

- General understanding of generative AI systems and text generation  
- Familiarity with basic workflow concepts (states, decisions, approvals)  
- Introductory experience reading or writing Mermaid diagrams  
- Awareness of common AI risks such as hallucinations, bias, unsafe content, or policy violations  

---

### Instructions

1. **Define the scenario**

   Choose a realistic content generation scenario where AI outputs require human oversight before use. Examples include:
   - Performance or promotion narratives  
   - Customer-facing communications  
   - Policy or compliance-related summaries  
   - Hiring, evaluation, or disciplinary documentation  

   Briefly describe the scenario and explain why unreviewed AI output would pose risk.

2. **Identify review points**

   Determine where human review should occur in the workflow. Consider:
   - Why this checkpoint is necessary  
   - What risks are being controlled at this stage  
   - Who is responsible for the review (for example, manager, legal reviewer, compliance officer)

3. **Design the core workflow**

   Create a Mermaid diagram that includes:
   - AI content generation  
   - A human review checkpoint  
   - Possible reviewer actions:
     - Approve  
     - Edit  
     - Reject  
     - Escalate  

   The diagram should clearly show how content flows between steps and how each decision affects the next action.

4. **Extend the workflow with feedback loops**

   Expand the diagram to show downstream outcomes, such as:
   - Approved content being published or stored  
   - Edited content triggering revision or regeneration  
   - Rejected content being blocked or logged  
   - Escalated content being routed to a higher authority or specialist  

5. **Annotate for accountability and safety**

   Add annotations to the Mermaid diagram that explain:
   - Which risks are being mitigated at each step  
   - How accountability is assigned and traceable  
   - How humans and AI collaborate safely without over-reliance on automation  

6. **Write a short explanation**

   Accompany the diagram with a concise written explanation that describes:
   - Why the workflow is designed this way  
   - How review decisions are made  
   - How escalation paths prevent unsafe or inappropriate outcomes  
   - How human oversight improves trust and reliability in the system  

---

### Deliverable

**Deliverable:**  
A documented human-in-the-loop workflow that includes:
- A Mermaid diagram modeling AI generation, human review, decision paths, and feedback loops  
- A short written explanation describing review decisions, escalation paths, and how human oversight reduces risk in a generative AI system
