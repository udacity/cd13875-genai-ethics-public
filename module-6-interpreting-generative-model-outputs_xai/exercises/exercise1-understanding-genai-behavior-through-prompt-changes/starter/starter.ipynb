{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from textwrap import shorten\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8850d",
   "metadata": {},
   "source": [
    "### Load provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1be72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load the dataset\n",
    "# -----------------------------\n",
    "\n",
    "# TODO: Update this path if your file name/location is different\n",
    "DATA_PATH = \"outputs.csv\"\n",
    "\n",
    "# TODO: Read the CSV into a dataframe called df\n",
    "# df = ...\n",
    "\n",
    "# TODO: Quick sanity checks (keep these simple)\n",
    "# - display the first 5 rows\n",
    "# - print the number of rows\n",
    "# - print the prompt types present\n",
    "\n",
    "# TODO: Verify required columns exist (if this fails, fix your CSV or path)\n",
    "# required_cols = [\"scenario_id\", \"product\", \"prompt_type\", \"prompt\", \"output\"]\n",
    "# missing = ...\n",
    "# if missing: raise ValueError(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775d958",
   "metadata": {},
   "source": [
    "### Establish baseline\n",
    "#### Review the baseline prompts and outputs to understand the neutral behavior of the model. Pay attention to tone, claim strength, and any implicit assumptions in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe833d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Establish a baseline (neutral prompt + output)\n",
    "# For each scenario_id, show the baseline prompt and output\n",
    "# ---------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TODO: Establish a baseline (neutral prompt + output)\n",
    "# For each scenario_id, show the baseline prompt and output.\n",
    "#\n",
    "# Requirements:\n",
    "# - Filter to prompt_type == \"baseline\"\n",
    "# - Sort by scenario_id\n",
    "# - Display these columns: scenario_id, product, prompt, output\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# TODO: Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41697e",
   "metadata": {},
   "source": [
    "### Analyze **prompt sensitivity** by comparing baseline outputs to outputs generated from prompts with a single small change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92188ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Prompt sensitivity analysis\n",
    "# Compare baseline vs sensitivity for each scenario_id\n",
    "# ---------------------------------------\n",
    "\n",
    "baseline = df[df[\"prompt_type\"] == \"baseline\"][[\"scenario_id\", \"product\", \"prompt\", \"output\"]].rename(\n",
    "    columns={\"prompt\": \"baseline_prompt\", \"output\": \"baseline_output\"}\n",
    ")\n",
    "\n",
    "sensitivity = df[df[\"prompt_type\"] == \"sensitivity\"][[\"scenario_id\", \"prompt\", \"output\"]].rename(\n",
    "    columns={\"prompt\": \"sensitivity_prompt\", \"output\": \"sensitivity_output\"}\n",
    ")\n",
    "\n",
    "sens_cmp = (\n",
    "    baseline\n",
    "    .merge(sensitivity, on=\"scenario_id\", how=\"inner\")\n",
    "    .sort_values([\"scenario_id\"])\n",
    ")\n",
    "\n",
    "display(sens_cmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa9f42",
   "metadata": {},
   "source": [
    "### Perform **counterfactual comparisons** using paired prompts that differ by one controlled change only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Counterfactual comparisons (single controlled change)\n",
    "# Compare counterfactual_a vs counterfactual_b for each scenario_id\n",
    "# ---------------------------------------\n",
    "\n",
    "cf_a = df[df[\"prompt_type\"] == \"counterfactual_a\"][[\"scenario_id\", \"product\", \"prompt\", \"output\"]].rename(\n",
    "    columns={\"prompt\": \"prompt_a\", \"output\": \"output_a\"}\n",
    ")\n",
    "\n",
    "cf_b = df[df[\"prompt_type\"] == \"counterfactual_b\"][[\"scenario_id\", \"product\", \"prompt\", \"output\"]].rename(\n",
    "    columns={\"prompt\": \"prompt_b\", \"output\": \"output_b\"}\n",
    ")\n",
    "\n",
    "cf_cmp = (\n",
    "    cf_a[[\"scenario_id\", \"product\", \"prompt_a\", \"output_a\"]]\n",
    "    .merge(cf_b[[\"scenario_id\", \"prompt_b\", \"output_b\"]], on=\"scenario_id\", how=\"inner\")\n",
    "    .sort_values([\"scenario_id\"])\n",
    ")\n",
    "\n",
    "display(cf_cmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Describe what the controlled change triggered (simple helper)\n",
    "# This prints a short, friendly comparison per scenario.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TODO 4 (optional but helpful): Print-friendly counterfactual viewer\n",
    "# Create a helper function that prints, for one row:\n",
    "# - scenario_id, product\n",
    "# - prompt_a and prompt_b\n",
    "# - output_a and output_b\n",
    "#\n",
    "# Then loop through your counterfactual table and print each scenario.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# TODO: Write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2526802",
   "metadata": {},
   "source": [
    "### Conduct quantitative analysis on the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29066c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Quantitative analysis (two simple approaches)\n",
    "# 1) Count/flag high-risk claims\n",
    "# 2) Count certainty/exaggeration language\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "HIGH_RISK_PHRASES = [\n",
    "    \"guarantee\", \"guaranteed\",\n",
    "    \"clinically tested\", \"clinically proven\",\n",
    "    \"medical-grade\", \"dermatologist-approved\",\n",
    "    \"fda-approved\", \"cure\", \"cures\", \"treat\", \"treats\", \"diagnose\"\n",
    "]\n",
    "\n",
    "CERTAINTY_WORDS = [\n",
    "    \"always\", \"never\", \"perfect\", \"proven\", \"unbeatable\", \"ultimate\", \"flawless\"\n",
    "]\n",
    "\n",
    "def count_phrases(text, phrases):\n",
    "    t = str(text).lower()\n",
    "    return sum(t.count(p) for p in phrases)\n",
    "\n",
    "def count_words(text, words):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", str(text).lower())\n",
    "    return sum(tokens.count(w) for w in words)\n",
    "\n",
    "# Add two simple metrics\n",
    "df[\"high_risk_count\"] = df[\"output\"].apply(lambda x: count_phrases(x, HIGH_RISK_PHRASES))\n",
    "df[\"certainty_count\"] = df[\"output\"].apply(lambda x: count_words(x, CERTAINTY_WORDS))\n",
    "\n",
    "# Simple flags (0/1) for easier interpretation\n",
    "df[\"has_high_risk\"] = (df[\"high_risk_count\"] > 0).astype(int)\n",
    "df[\"has_certainty\"] = (df[\"certainty_count\"] > 0).astype(int)\n",
    "\n",
    "# Row-level view (quick scan)\n",
    "display(df[[\"scenario_id\", \"product\", \"prompt_type\", \"high_risk_count\", \"certainty_count\", \"has_high_risk\", \"has_certainty\"]])\n",
    "\n",
    "# Group summary (compare baseline/sensitivity/counterfactuals)\n",
    "summary = (\n",
    "    df.groupby([\"scenario_id\", \"prompt_type\"])[[\"high_risk_count\", \"certainty_count\", \"has_high_risk\", \"has_certainty\"]]\n",
    "      .mean()\n",
    "      .round(2)\n",
    "      .reset_index()\n",
    "      .sort_values([\"scenario_id\", \"prompt_type\"])\n",
    ")\n",
    "\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da58fe7",
   "metadata": {},
   "source": [
    "### Identify and document any **unexpected behaviors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb2d24",
   "metadata": {},
   "source": [
    "### Manual Review: Identify Unexpected or Risky Model Behaviors\n",
    "\n",
    "In this step, **do not write new code**.\n",
    "\n",
    "Carefully review the generated outputs in your notebook and identify any **unexpected behaviors** that would require escalation before deployment.\n",
    "\n",
    "As you read through each output, look for issues such as:\n",
    "\n",
    "- **Hallucinated features or specifications**  \n",
    "  Claims about capabilities, ingredients, or technical details that were not provided in the prompt or are unlikely to be verifiable.\n",
    "\n",
    "- **Unsafe or misleading advice or claims**  \n",
    "  Absolute guarantees, medical or regulatory claims, or statements that could mislead users.\n",
    "\n",
    "- **Inconsistent or contradictory safety language**  \n",
    "  For example, outputs that state a product is “safe for children” but omit supervision guidance, or safety language that appears in some prompt variations but not others.\n",
    "\n",
    "- **Violations of stated constraints or brand voice**  \n",
    "  Outputs that ignore explicit instructions in the prompt (for example, “avoid medical claims”) or shift tone in a way that would violate brand or policy guidelines.\n",
    "\n",
    "For each issue you identify, document the following directly in the notebook:\n",
    "\n",
    "- What the unexpected behavior is  \n",
    "- Which prompt variation triggered it  \n",
    "- Why it could pose a risk in a real deployment  \n",
    "\n",
    "This manual review complements the quantitative analysis by capturing risks that simple metrics may miss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dcf513",
   "metadata": {},
   "source": [
    "### Summarize your findings directly in the notebook by creating an **explainability evidence table** that links prompt changes to observed behavioral shifts and highlights potential deployment risks\n",
    "#### TODO: Replace TBD values with your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Explainability Evidence Table\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Prompt Sensitivity: baseline vs sensitivity ---\n",
    "baseline = df[df[\"prompt_type\"] == \"baseline\"][\n",
    "    [\"scenario_id\", \"product\", \"prompt\", \"output\", \"high_risk_count\", \"certainty_count\"]\n",
    "].rename(columns={\n",
    "    \"prompt\": \"prompt_left\",\n",
    "    \"output\": \"output_left\",\n",
    "    \"high_risk_count\": \"high_risk_left\",\n",
    "    \"certainty_count\": \"certainty_left\"\n",
    "})\n",
    "\n",
    "sensitivity = df[df[\"prompt_type\"] == \"sensitivity\"][\n",
    "    [\"scenario_id\", \"prompt\", \"output\", \"high_risk_count\", \"certainty_count\"]\n",
    "].rename(columns={\n",
    "    \"prompt\": \"prompt_right\",\n",
    "    \"output\": \"output_right\",\n",
    "    \"high_risk_count\": \"high_risk_right\",\n",
    "    \"certainty_count\": \"certainty_right\"\n",
    "})\n",
    "\n",
    "sens_evidence = baseline.merge(sensitivity, on=\"scenario_id\", how=\"inner\")\n",
    "sens_evidence[\"comparison_type\"] = \"prompt_sensitivity\"\n",
    "sens_evidence[\"delta_high_risk\"] = sens_evidence[\"high_risk_right\"] - sens_evidence[\"high_risk_left\"]\n",
    "sens_evidence[\"delta_certainty\"] = sens_evidence[\"certainty_right\"] - sens_evidence[\"certainty_left\"]\n",
    "\n",
    "# TODO: Replace TBD with student written analysis\n",
    "sens_evidence[\"observed_behavior\"] = \"TBD\"\n",
    "sens_evidence[\"deployment_risk\"] = \"TBD\"\n",
    "\n",
    "sens_evidence = sens_evidence[[\n",
    "    \"comparison_type\",\n",
    "    \"scenario_id\",\n",
    "    \"product\",\n",
    "    \"prompt_left\",\n",
    "    \"prompt_right\",\n",
    "    \"delta_high_risk\",\n",
    "    \"delta_certainty\",\n",
    "    \"observed_behavior\",\n",
    "    \"deployment_risk\"\n",
    "]]\n",
    "\n",
    "# --- Counterfactuals: counterfactual_a vs counterfactual_b ---\n",
    "cf_a = df[df[\"prompt_type\"] == \"counterfactual_a\"][\n",
    "    [\"scenario_id\", \"product\", \"prompt\", \"output\", \"high_risk_count\", \"certainty_count\"]\n",
    "].rename(columns={\n",
    "    \"prompt\": \"prompt_left\",\n",
    "    \"output\": \"output_left\",\n",
    "    \"high_risk_count\": \"high_risk_left\",\n",
    "    \"certainty_count\": \"certainty_left\"\n",
    "})\n",
    "\n",
    "cf_b = df[df[\"prompt_type\"] == \"counterfactual_b\"][\n",
    "    [\"scenario_id\", \"prompt\", \"output\", \"high_risk_count\", \"certainty_count\"]\n",
    "].rename(columns={\n",
    "    \"prompt\": \"prompt_right\",\n",
    "    \"output\": \"output_right\",\n",
    "    \"high_risk_count\": \"high_risk_right\",\n",
    "    \"certainty_count\": \"certainty_right\"\n",
    "})\n",
    "\n",
    "cf_evidence = cf_a.merge(cf_b, on=\"scenario_id\", how=\"inner\")\n",
    "cf_evidence[\"comparison_type\"] = \"counterfactual_pair\"\n",
    "cf_evidence[\"delta_high_risk\"] = cf_evidence[\"high_risk_right\"] - cf_evidence[\"high_risk_left\"]\n",
    "cf_evidence[\"delta_certainty\"] = cf_evidence[\"certainty_right\"] - cf_evidence[\"certainty_left\"]\n",
    "\n",
    "# TODO: Replace TBD with student written analysis\n",
    "cf_evidence[\"observed_behavior\"] = \"TBD\"\n",
    "cf_evidence[\"deployment_risk\"] = \"TBD\"\n",
    "\n",
    "cf_evidence = cf_evidence[[\n",
    "    \"comparison_type\",\n",
    "    \"scenario_id\",\n",
    "    \"product\",\n",
    "    \"prompt_left\",\n",
    "    \"prompt_right\",\n",
    "    \"delta_high_risk\",\n",
    "    \"delta_certainty\",\n",
    "    \"observed_behavior\",\n",
    "    \"deployment_risk\"\n",
    "]]\n",
    "\n",
    "# --- Final Evidence Table ---\n",
    "explainability_evidence = (\n",
    "    pd.concat([sens_evidence, cf_evidence], ignore_index=True)\n",
    "    .sort_values([\"scenario_id\", \"comparison_type\"])\n",
    ")\n",
    "\n",
    "display(explainability_evidence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
