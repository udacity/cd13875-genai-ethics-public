{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e89a9d",
   "metadata": {},
   "source": [
    "### Load a provided set of simulated promotion-readiness outputs produced by a generative model across multiple employee scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49619e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load the dataset \n",
    "# -----------------------------\n",
    "# TODO:\n",
    "# 1) Update DATA_PATH if needed\n",
    "# 2) Load the CSV into a DataFrame named `df`\n",
    "# 3) Preview the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad741a7",
   "metadata": {},
   "source": [
    "### Review the scenarios and outputs to identify where language framing differs across otherwise similar performance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Review the lists below\n",
    "# - Add/remove terms based on what you observe in the dataset outputs\n",
    "\n",
    "# ---------------------------------------\n",
    "# Define simple signals we want to track\n",
    "# ---------------------------------------\n",
    "GENDER_TERMS = {\n",
    "    \"male\": [\"he\", \"him\", \"his\", \"man\", \"male\"],\n",
    "    \"female\": [\"she\", \"her\", \"hers\", \"woman\", \"female\"],\n",
    "}\n",
    "\n",
    "FRAMING_TERMS = {\n",
    "    \"confidence\": [\"confident\", \"assertive\", \"decisive\", \"driven\"],\n",
    "    \"doubt\": [\"uncertain\", \"hesitant\", \"not ready\", \"needs guidance\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text, terms):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", str(text).lower())\n",
    "    return sum(tokens.count(t) for t in terms)\n",
    "\n",
    "def count_phrases(text, phrases):\n",
    "    t = str(text).lower()\n",
    "    return sum(t.count(p) for p in phrases)\n",
    "\n",
    "def analyze(text):\n",
    "    t = str(text).replace(\"\\\\n\", \"\\n\")\n",
    "    return {\n",
    "        \"male_terms\": count_words(t, GENDER_TERMS[\"male\"]),\n",
    "        \"female_terms\": count_words(t, GENDER_TERMS[\"female\"]),\n",
    "        \"confidence_terms\": count_phrases(t, FRAMING_TERMS[\"confidence\"]),\n",
    "        \"doubt_terms\": count_phrases(t, FRAMING_TERMS[\"doubt\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e3ca0",
   "metadata": {},
   "source": [
    "### Quantitatively measure bias signals in the text using Python and Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Baseline analysis (before)\n",
    "# -----------------------------\n",
    "\n",
    "# TODO: Make this a reusable function.\n",
    "# Requirements:\n",
    "# - Apply `analyze()` to `metric_column` and join results back to the original dataframe\n",
    "# - Show a row-level preview\n",
    "# - Create a grouped summary table (mean of each metric)\n",
    "# - Return the summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a6ca5",
   "metadata": {},
   "source": [
    "### Apply mitigation strategies appropriate for GenAI outputs\n",
    "#### Add post-processing filters that remove or normalize biased wording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Simple mitigation (post-processing only)\n",
    "#    - replace gendered pronouns with \"they\"\n",
    "# -----------------------------------------\n",
    "\n",
    "# TODO: Implement a lightweight mitigation function.\n",
    "# Requirements:\n",
    "# - Start from the input text\n",
    "# - Neutralize gendered pronouns (he/him/his, she/her/hers)\n",
    "# - Optional: normalize simple grammar where needed (ex: is -> are) if your rewrite uses \"they\"\n",
    "# - Tidy extra blank lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfd434",
   "metadata": {},
   "source": [
    "### Re-run the same outputs after mitigation and compare results against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Run `run_bias_metrics` on the mitigated column\n",
    "# - Create a comparison table showing baseline vs mitigated side-by-side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972d175",
   "metadata": {},
   "source": [
    "### Document how mitigation strategies altered language patterns and whether they reduced biased framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ed6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# TODO: Create an Explainability Summary\n",
    "# -----------------------------------------\n",
    "# Merge the \"before\" and \"after\" summary tables so that bias metrics\n",
    "# can be compared side by side for each performance pattern.\n",
    "#\n",
    "# HINT:\n",
    "# - Use a merge/join operation on the performance pattern column.\n",
    "# - The resulting table should include both baseline and mitigated metrics.\n",
    "#\n",
    "# TODO:\n",
    "# explainability_summary = ...\n",
    "\n",
    "# TODO:\n",
    "# Display the explainability summary table so you can visually compare\n",
    "# bias signals before and after mitigation.\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# TODO: Compare Before vs After Outputs\n",
    "# -----------------------------------------\n",
    "# Select at least two examples from the dataset and print:\n",
    "# - The performance pattern\n",
    "# - The original (baseline) generated output\n",
    "# - The mitigated version of the output\n",
    "#\n",
    "# HINT:\n",
    "# - Loop over a small subset of rows (for example, the first two).\n",
    "# - Format the output so it is easy to read and compare.\n",
    "#\n",
    "# TODO:\n",
    "# for each selected example:\n",
    "#     print performance pattern\n",
    "#     print BEFORE output\n",
    "#     print AFTER output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
