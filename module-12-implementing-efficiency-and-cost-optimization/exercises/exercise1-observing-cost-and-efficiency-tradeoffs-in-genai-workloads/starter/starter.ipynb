{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ded844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32040b0",
   "metadata": {},
   "source": [
    "### Review the workload context and usage assumptions provided in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Review the workload context and usage assumptions provided in the notebook.\n",
    "# ---------------------------------------------------------\n",
    "# TODO:\n",
    "# 1) Create a dictionary called `workload_context` with the following keys:\n",
    "#    - workload_name\n",
    "#    - traffic_pattern\n",
    "#    - requests_per_day\n",
    "#    - avg_input_tokens\n",
    "#    - avg_output_tokens\n",
    "#    - sla_notes\n",
    "#    - kg_co2e_per_kwh\n",
    "# 3) Display the dictionary as a one-row DataFrame.\n",
    "\n",
    "# workload_context = {\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# display(pd.DataFrame([workload_context]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6bfc7",
   "metadata": {},
   "source": [
    "### Load the baseline inference metrics for the simulated generative AI workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9392dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Instruction Step 3\n",
    "# Load the baseline inference metrics for the simulated workload.\n",
    "# ---------------------------------------------------------\n",
    "baseline_path = Path(\"baseline_inference_metrics.csv\")\n",
    "\n",
    "# TODO:\n",
    "# 1) Add a simple check that raises a helpful error if the CSV is not found.\n",
    "# 2) Read the CSV into a DataFrame called `baseline_df`.\n",
    "# 3) Display `baseline_df`.\n",
    "\n",
    "# if not baseline_path.exists():\n",
    "#     raise FileNotFoundError(\"...\")\n",
    "\n",
    "# baseline_df = pd.read_csv(baseline_path)\n",
    "# display(baseline_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d8c06",
   "metadata": {},
   "source": [
    "### Analyze the baseline metrics to identify potential efficiency concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a229de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull baseline values into a dict for easy downstream calculations.\n",
    "baseline = dict(zip(baseline_df[\"metric\"], baseline_df[\"baseline_value\"]))\n",
    "\n",
    "# A few simple checks based on the workload notes.\n",
    "baseline_findings = []\n",
    "\n",
    "p95 = baseline.get(\"p95_latency_ms\")\n",
    "if p95 is not None:\n",
    "    if p95 > 1200:\n",
    "        baseline_findings.append(f\"p95 latency is {p95:.0f} ms, which may be high for an interactive SLA.\")\n",
    "    else:\n",
    "        baseline_findings.append(f\"p95 latency is {p95:.0f} ms, within the stated interactive target range.\")\n",
    "\n",
    "mem = baseline.get(\"average_memory_gb\")\n",
    "if mem is not None:\n",
    "    if mem >= 24:\n",
    "        baseline_findings.append(f\"Average memory is {mem:.1f} GB, which may force larger (more expensive) GPUs.\")\n",
    "    elif mem >= 16:\n",
    "        baseline_findings.append(f\"Average memory is {mem:.1f} GB; watch GPU sizing and headroom for spikes.\")\n",
    "    else:\n",
    "        baseline_findings.append(f\"Average memory is {mem:.1f} GB, leaving reasonable headroom for common GPU tiers.\")\n",
    "\n",
    "cost = baseline.get(\"cost_per_1k_requests_usd\")\n",
    "if cost is not None:\n",
    "    baseline_findings.append(f\"Estimated cost is ${cost:.2f} per 1k requests. Scale impact depends on daily volume.\")\n",
    "\n",
    "energy = baseline.get(\"estimated_energy_kwh_per_1k_requests\")\n",
    "if energy is not None:\n",
    "    baseline_findings.append(f\"Estimated energy is {energy:.2f} kWh per 1k requests. Consider energy as a constraint at scale.\")\n",
    "\n",
    "pd.DataFrame({\"baseline_findings\": baseline_findings})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b541a8",
   "metadata": {},
   "source": [
    "### Review the simulated optimization scenario and its post-optimization metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cd9cd",
   "metadata": {},
   "source": [
    "#### Optimization Scenario \n",
    "\n",
    "For this exercise, assume the following optimization has already been applied to the generative AI inference system.\n",
    "\n",
    "##### Optimization Applied\n",
    "**INT8 quantization with dynamic batching**\n",
    "\n",
    "##### Description\n",
    "- Reduced-precision (INT8) inference is used to lower memory usage and improve throughput.\n",
    "- Dynamic batching groups multiple requests together to increase hardware utilization under steady traffic.\n",
    "\n",
    "##### Expected Benefits\n",
    "- Lower GPU memory footprint\n",
    "- Higher sustained throughput\n",
    "- Reduced cost per request\n",
    "- Reduced energy consumption per request\n",
    "\n",
    "##### Known Tradeoffs\n",
    "- Median latency may increase slightly due to batching overhead\n",
    "- Output quality impact is expected to be minor but not zero\n",
    "- Occasional formatting drift or subtle tone changes may occur for longer outputs\n",
    "\n",
    "##### Your Task\n",
    "You are **not** asked to design or implement this optimization.\n",
    "\n",
    "Your task is to **evaluate whether this optimization is acceptable** for the workload using the provided performance, cost, and energy metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0057bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_scenario = {\n",
    "    \"name\": \"INT8 quantization + dynamic batching\",\n",
    "    \"notes\": [\n",
    "        \"Reduced precision inference lowers memory and can improve throughput.\",\n",
    "        \"Batching can increase throughput but may increase median latency under certain traffic patterns.\",\n",
    "        \"Quality impact is assumed to be minimal but not zero (monitor formatting + factuality).\",\n",
    "    ],\n",
    "    \"quality_impact_note\": \"Minor: occasional slight tone shift and rare formatting drift under long outputs.\",\n",
    "}\n",
    "\n",
    "pd.DataFrame({\"scenario_notes\": optimization_scenario[\"notes\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba56c9d",
   "metadata": {},
   "source": [
    "### Construct a comparison table that summarizes baseline and optimized metrics side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170dd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Construct a comparison table (baseline vs optimized).\n",
    "# ---------------------------------------------------------\n",
    "# TODO:\n",
    "# 1) Create a dictionary called `optimized_metrics` with the SAME metric names as baseline_df[\"metric\"].\n",
    "#    The values should represent the optimized scenario.\n",
    "# 2) Convert it into a DataFrame called `optimized_df` with columns:\n",
    "#    - metric\n",
    "#    - optimized_value\n",
    "# 3) Merge baseline_df and optimized_df into `comparison`.\n",
    "# 4) Add:\n",
    "#    - delta (optimized - baseline)\n",
    "#    - pct_change ((delta / baseline) * 100)\n",
    "\n",
    "# optimized_metrics = {\n",
    "#     ...\n",
    "# }\n",
    "# optimized_df = pd.DataFrame({\n",
    "#     \"metric\": ...,\n",
    "#     \"optimized_value\": ...\n",
    "# })\n",
    "# comparison = baseline_df.merge(optimized_df, on=\"metric\", how=\"left\")\n",
    "# comparison[\"delta\"] = ...\n",
    "# comparison[\"pct_change\"] = ...\n",
    "# display(comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade38d79",
   "metadata": {},
   "source": [
    "### Calculate and summarize the differences between baseline and optimized metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_per_day = workload_context[\"requests_per_day\"]\n",
    "\n",
    "# Pull per-1k request cost/energy and scale them\n",
    "baseline_cost_per_day = (baseline[\"cost_per_1k_requests_usd\"] * requests_per_day) / 1000\n",
    "optimized_cost_per_day = (optimized_metrics[\"cost_per_1k_requests_usd\"] * requests_per_day) / 1000\n",
    "\n",
    "baseline_energy_per_day = (baseline[\"estimated_energy_kwh_per_1k_requests\"] * requests_per_day) / 1000\n",
    "optimized_energy_per_day = (optimized_metrics[\"estimated_energy_kwh_per_1k_requests\"] * requests_per_day) / 1000\n",
    "\n",
    "# Optional CO2e estimate (assumption-based)\n",
    "kg_co2e_per_kwh = workload_context.get(\"kg_co2e_per_kwh\", None)\n",
    "baseline_co2e_kg_per_day = baseline_energy_per_day * kg_co2e_per_kwh if kg_co2e_per_kwh is not None else None\n",
    "optimized_co2e_kg_per_day = optimized_energy_per_day * kg_co2e_per_kwh if kg_co2e_per_kwh is not None else None\n",
    "\n",
    "scale_summary = {\n",
    "    \"requests_per_day\": requests_per_day,\n",
    "    \"baseline_cost_per_day_usd\": baseline_cost_per_day,\n",
    "    \"optimized_cost_per_day_usd\": optimized_cost_per_day,\n",
    "    \"daily_cost_savings_usd\": baseline_cost_per_day - optimized_cost_per_day,\n",
    "    \"baseline_energy_kwh_per_day\": baseline_energy_per_day,\n",
    "    \"optimized_energy_kwh_per_day\": optimized_energy_per_day,\n",
    "    \"daily_energy_savings_kwh\": baseline_energy_per_day - optimized_energy_per_day,\n",
    "}\n",
    "\n",
    "if baseline_co2e_kg_per_day is not None:\n",
    "    scale_summary.update({\n",
    "        \"assumed_kg_co2e_per_kwh\": kg_co2e_per_kwh,\n",
    "        \"baseline_co2e_kg_per_day\": baseline_co2e_kg_per_day,\n",
    "        \"optimized_co2e_kg_per_day\": optimized_co2e_kg_per_day,\n",
    "        \"daily_co2e_savings_kg\": baseline_co2e_kg_per_day - optimized_co2e_kg_per_day,\n",
    "    })\n",
    "\n",
    "pd.DataFrame([scale_summary]).T.rename(columns={0: \"value\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c46e43",
   "metadata": {},
   "source": [
    "### Evaluate the tradeoffs introduced by the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeoffs = []\n",
    "\n",
    "# Latency tradeoff (p50 may increase; p95 may improve)\n",
    "tradeoffs.append({\n",
    "    \"area\": \"Latency\",\n",
    "    \"what_changed\": f\"p50: {baseline['p50_latency_ms']} -> {optimized_metrics['p50_latency_ms']} ms, \"\n",
    "                    f\"p95: {baseline['p95_latency_ms']} -> {optimized_metrics['p95_latency_ms']} ms\",\n",
    "    \"interpretation\": \"Median latency slightly increased (batching overhead), but tail latency improved.\"\n",
    "})\n",
    "\n",
    "# Throughput\n",
    "tradeoffs.append({\n",
    "    \"area\": \"Throughput\",\n",
    "    \"what_changed\": f\"{baseline['throughput_requests_per_sec']} -> {optimized_metrics['throughput_requests_per_sec']} req/s\",\n",
    "    \"interpretation\": \"Higher throughput reduces queueing risk and can stabilize tail latency under load.\"\n",
    "})\n",
    "\n",
    "# Memory\n",
    "tradeoffs.append({\n",
    "    \"area\": \"Memory\",\n",
    "    \"what_changed\": f\"{baseline['average_memory_gb']} -> {optimized_metrics['average_memory_gb']} GB\",\n",
    "    \"interpretation\": \"Lower memory enables smaller instances or more headroom, improving cost flexibility.\"\n",
    "})\n",
    "\n",
    "# Quality / flexibility (scenario-based note)\n",
    "tradeoffs.append({\n",
    "    \"area\": \"Output Quality\",\n",
    "    \"what_changed\": optimization_scenario[\"quality_impact_note\"],\n",
    "    \"interpretation\": \"Treat as a monitoring requirement. Add regression tests for tone, formatting, and factuality.\"\n",
    "})\n",
    "\n",
    "pd.DataFrame(tradeoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a0497",
   "metadata": {},
   "source": [
    "### Short analysis and clear recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd098981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Write a short analysis summarizing wins, risks, and acceptability.\n",
    "# ---------------------------------------------------------\n",
    "# TODO:\n",
    "# Write a short markdown-style summary (as a Python multi-line string)\n",
    "# that includes:\n",
    "# - the biggest improvements (cost, energy, throughput, etc.)\n",
    "# - the biggest tradeoffs (latency, quality, flexibility)\n",
    "# - whether the optimization is acceptable for this workload, and why\n",
    "#\n",
    "# Tip: Use f-strings and the baseline/optimized values.\n",
    "\n",
    "# analysis = f\"\"\"\n",
    "# ...\n",
    "# \"\"\".strip()\n",
    "# print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969cb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Conclude with a clear recommendation supported by the data.\n",
    "# ---------------------------------------------------------\n",
    "# TODO:\n",
    "# Create a dictionary called `recommendation` with:\n",
    "# - decision: one of [\"Adopt\", \"Adopt with guardrails\", \"Do not adopt\"]\n",
    "# - guardrails: list of strings (if applicable)\n",
    "# - why: short justification tied to your results\n",
    "#\n",
    "# Display it as a one-row DataFrame.\n",
    "\n",
    "# recommendation = {\n",
    "#     \"decision\": \"...\",\n",
    "#     \"guardrails\": [\"...\", \"...\"],\n",
    "#     \"why\": \"...\"\n",
    "# }\n",
    "# display(pd.DataFrame({\n",
    "#     \"decision\": [recommendation[\"decision\"]],\n",
    "#     \"why\": [recommendation[\"why\"]],\n",
    "#     \"guardrails\": [\"; \".join(recommendation.get(\"guardrails\", []))]\n",
    "# }))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
